{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Developing Custom Image Classification Model\n\nIn this lab, you will developed a custom image classification model to automatically classify the type of land shown in aerial images of 224-meter x 224-meter plots. Land use classification models can be used to track urbanization, deforestation, loss of wetlands, and other major environmental trends using periodically collected aerial imagery. The images used in this lab are based off of imagery from the U.S. National Land Cover Database. U.S. National Land Cover Database defines six primary classes of land use: *Developed*, *Barren*, *Forested*, *Grassland*, *Shrub*, *Cultivated*. Example images from each land use class are shown here:\n\nDeveloped | Cultivated | Barren\n--------- | ------ | ----------\n![Developed](https://github.com/jakazmie/images-for-hands-on-labs/raw/master/developed1.png) | ![Cultivated](https://github.com/jakazmie/images-for-hands-on-labs/raw/master/cultivated1.png) | ![Barren](https://github.com/jakazmie/images-for-hands-on-labs/raw/master/barren1.png)\n\nForested | Grassland | Shrub\n---------| ----------| -----\n![Forested](https://github.com/jakazmie/images-for-hands-on-labs/raw/master/forest1.png) | ![Grassland](https://github.com/jakazmie/images-for-hands-on-labs/raw/master/grassland1.png) | ![Shrub](https://github.com/jakazmie/images-for-hands-on-labs/raw/master/shrub1.png)\n\nYou shall employ a machine learning technique called transfer learning. Transfer learning is one of the fastest (code and run-time-wise) ways to start using deep learning. It allows for the reuse of knowledge gained while solving one problem to a different but related problem. For example, knowledge gained while learning to recognize landmarks and landscapes could apply when trying to recognize aerial land plots. Transfer Learning makes it feasible to train very effective ML models on relatively small training data sets.\n\nAlthough the primary goal of this lab is to understand how to use Azure ML to orchestrate deep learning workflows rather then to dive into Deep Learning techniques, ask the instructor if you want to better understand the approach utilized in the lab.\n\nYou will start by pre-processing training images into a set of powerful features - sometimes referred to as bottleneck features.\n\nTo create bottleneck features you will utilize a pre-trained Deep Learning network that was trained on a general computer vision domain. \n\nAlthough, the pre-trained network does not know how to classify aerial land plot images, it knows enough about representing image concepts that if we use it to pre-process aerial images, the extracted image features can be used to effectively train a relatively simple classifier on a **limited number** of samples.\n\nThe below diagram represents the architecture of our solution.\n\n![Transfer Learning](https://github.com/jakazmie/images-for-hands-on-labs/raw/master/TLArch.png)\n\nWe will use **ResNet50** trained on **imagenet** dataset to extract features. We will occasionally refer to this component of the solution as a featurizer. The output of the featurizer is a vector of 2048 floating point numbers, each representing a feature extracted from an image. \n\nWe will then use extracted features to train a simple fully connected neural network (the top) that will peform final image classification.\n\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Connect to AML Workspace"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Check core SDK version number\nimport azureml.core\nprint(\"SDK version:\", azureml.core.VERSION)",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "SDK version: 1.0.8\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import azureml.core\nfrom azureml.core import Workspace\n\nws = Workspace.from_config()\nprint(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Found the config file in: /home/nbuser/library/aml_config/config.json\njkamlws\njkamlws\nsouthcentralus\n952a710c-8d9c-40c1-9fec-f752138cc0b3\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Create Azure ML Managed Compute\n\nTo run the lab's scripts we will utilize Azure ML managed compute resources. Specifically, an autoscale cluster of *Standard_NC6* VMs (equipped with Tesla K80 GPU). "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.compute import AmlCompute\nfrom azureml.core.compute import ComputeTarget\nimport os\n\n\n# choose a name for your cluster\ncompute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"gpu-cluster\")\ncompute_min_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MIN_NODES\", 1)\ncompute_max_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MAX_NODES\", 4)\n\nvm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_NC6\")\n#vm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_ND6s\")\n\nif compute_name in ws.compute_targets:\n    compute_target = ws.compute_targets[compute_name]\n    if compute_target and type(compute_target) is AmlCompute:\n        print('found compute target. just use it. ' + compute_name)\nelse:\n    print('creating a new compute target...')\n    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size,\n                                                                min_nodes = compute_min_nodes, \n                                                                max_nodes = compute_max_nodes)\n\n    # create the cluster\n    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n\n    # can poll for a minimum number of nodes and for a specific timeout. \n    # if no min node count is provided it will use the scale settings for the cluster\n    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n\n     # For a more detailed view of current AmlCompute status, use the 'status' property    \n    print(compute_target.status.serialize())",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": "creating a new compute target...\nCreating\nSucceeded.......................\nAmlCompute wait for completion finished\nMinimum number of nodes requested have been provisioned\n{'allocationState': 'Steady', 'allocationStateTransitionTime': '2019-02-04T16:56:43.250000+00:00', 'creationTime': '2019-02-04T16:54:21.524384+00:00', 'currentNodeCount': 1, 'errors': None, 'modifiedTime': '2019-02-04T16:54:39.651804+00:00', 'nodeStateCounts': {'idleNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0, 'preparingNodeCount': 1, 'runningNodeCount': 0, 'unusableNodeCount': 0}, 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 1, 'maxNodeCount': 4, 'nodeIdleTimeBeforeScaleDown': 'PT120S'}, 'targetNodeCount': 1, 'vmPriority': 'Dedicated', 'vmSize': 'STANDARD_NC6'}\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Feature Extraction\n\nAs noted in the introduction, we will utilize a pretrained **ResNet50** convolutional neural net as a featurizer. This CNN was trained on *Imagenet* dataset. \n\nThe feature extraction code is encapsulated in a Python script that will be executed on a remote Azure ML Compute.\n\nThe script processes an input image datasets into an output bottleneck feature sets. The script expects the images to be organized in the below folder structure:\n```\nBarren/\nCultivated/\nDeveloped/\nForest/\nHerbaceous/\nShrub/\n```\n\nThe location of the input dataset and the location where to save the output dataset are passed to the script as command line parameters. The output dataset will be stored in a binary HDF5 data format used commonly in Machine Learning and High Performance Computing solutions.\n\n\nWe will not attempt to run the script on a full dataset in a local environment. It is very computationally intensive and unless you run it in an evironment equipped with a powerful GPU it would be very slow. \n\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Create a feature extraction script"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os\nscript_folder = './script'\nos.makedirs(script_folder, exist_ok=True)",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%writefile $script_folder/extract.py\n\nimport os\nimport numpy as np\nimport random\nimport h5py\n\nimport tensorflow as tf\nfrom tensorflow.keras.applications import resnet50\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.utils import to_categorical\n\n\ndef create_bottleneck_features(data_dir, output_file, featurizer):\n    \n    # A hack to mitigate a bug in TF.Keras 1.12\n    def preprocess_input_new(x):\n        img = resnet50.preprocess_input(image.img_to_array(x))\n        return image.array_to_img(img)\n    \n    # Create a Keras generator to read and pre-process images. The generator\n    # will return batches of numpy arrays representing pre-processed images\n    batchsize=64\n    classes = [\"Barren\", \"Cultivated\", \"Developed\", \"Forest\", \"Herbaceous\", \"Shrub\"]\n    datagen = ImageDataGenerator(preprocessing_function=preprocess_input_new)\n    generator = datagen.flow_from_directory(\n        directory=data_dir,\n        target_size=(224, 224),\n        classes=classes,\n        batch_size=batchsize)\n\n    # Generate bottleneck features\n    # Due to the bug in Tensorflow 1.12 we cannot use predict_generator \n    # Instead we are invoking model.predict in an explicit loop\n    features = []\n    labels = []\n    batches = len(generator)\n    for i in range(batches):\n        image_batch, label_batch = generator.next()\n        features.extend(featurizer.predict(image_batch, batch_size=batchsize))\n        labels.extend(label_batch)\n        print(\"Processed batch: {} out of {}\".format(i+1, batches))\n        \n    features = np.asarray(features)\n    labels = np.asarray(labels)   \n \n    # Save dataset to HDF5 file\n    print(\"Saving features to {}\".format(output_file))\n    print(\"   Features: \", features.shape)\n    print(\"   Labels: \", labels.shape)\n    with h5py.File(output_file, \"w\") as hfile:\n        features_dset = hfile.create_dataset('features', data=features)\n        labels_dset = hfile.create_dataset('labels', data=labels)\n\n\nFLAGS = tf.app.flags.FLAGS\n\n# Default global parameters\ntf.app.flags.DEFINE_integer('batch_size', 64, \"Number of images per batch\")\ntf.app.flags.DEFINE_string('data_folder', 'aerialsmall', \"Folder with training and validation images\")\ntf.app.flags.DEFINE_string('output_data_dir', 'bottleneck', \"A folder for saving bottleneck features\")\ntf.app.flags.DEFINE_string('output_file_suffix', 'aerial_bottleneck_keras.h5', \"Filename template for output features\")\n\n\ndef main(argv=None):\n    print(\"Starting\")\n    print(\"Reading training data from:\", FLAGS.data_folder)\n    print(\"Output bottleneck files will be saved to:\", FLAGS.output_data_dir)\n    os.makedirs(FLAGS.output_data_dir, exist_ok=True)\n   \n    train_data_dir = os.path.join(FLAGS.data_folder, 'train')\n    valid_data_dir = os.path.join(FLAGS.data_folder, 'valid')\n    \n    train_output_file = os.path.join(FLAGS.output_data_dir, 'train_' + FLAGS.output_file_suffix)\n    valid_output_file = os.path.join(FLAGS.output_data_dir, 'valid_' + FLAGS.output_file_suffix)\n\n    # Create a featurizer\n    featurizer = resnet50.ResNet50(\n                weights = 'imagenet', \n                input_shape=(224,224,3), \n                include_top = False,\n                pooling = 'avg')\n\n    print(\"Creating training bottleneck features\")\n    create_bottleneck_features(train_data_dir, train_output_file, featurizer)\n    print(\"Creating validation bottleneck features\")\n    create_bottleneck_features(valid_data_dir, valid_output_file, featurizer)\n  \nif __name__ == '__main__':\n    tf.app.run()",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Writing ./script/extract.py\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Configure Datastores \nThe training images have been uploaded to a public Azure blob storage container. We will register this container as an AML Datastore within our workspace. Before the data prep script runs, the datastore's content - training images - will be copied to the local storage on compute nodes.\n\nAfter the script completes, its output - the bottleneck features file - will be uploaded by AML to the workspace's default datastore."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core import Datastore\n\nimages_account = 'azureailabs'\nimages_container = 'aerialsmall'\ndatastore_name = 'input_data'\n\n# Check if the datastore exists. If not create a new one\ntry:\n    input_ds = Datastore.get(ws, datastore_name)\n    print('Found existing datastore for input images:', input_ds.name)\nexcept:\n    input_ds = Datastore.register_azure_blob_container(workspace=ws, datastore_name=datastore_name,\n                                            container_name=images_container,\n                                            account_name=images_account)\n    print('Creating new datastore for input images')\n\n \n   \nprint(input_ds.name, input_ds.datastore_type, input_ds.account_name, input_ds.container_name)\n\noutput_ds = ws.get_default_datastore()\nprint(\"Using the default datastore for output: \")\nprint(output_ds.name, output_ds.datastore_type, output_ds.account_name, output_ds.container_name)\n",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Creating new datastore for input images\ninput_data AzureBlob azureailabs aerialsmall\nUsing the default datastore for output: \nworkspaceblobstore AzureBlob jkamlwsstorageijhxtdoi azureml-blobstore-68c45b05-3479-49da-8b04-2517f76acaf6\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Create AML Experiment\nWe will track runs of the feature extraction script in a dedicated Experiment."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core import Experiment\nexperiment_name = 'aerial-feature-extraction'\nexp = Experiment(workspace=ws, name=experiment_name)",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Start and monitor a remote run\n\nWe will run a script on a single node in a docker container. The docker image will be configured and created using AML APIs.\n\nThe first run takes longer. The subsequent runs, as long as the script dependencies don't change, are much faster.\n\nYou can check the progress of a running job in multiple ways: Azure Portal, AML Jupyter Widgets, log files streaming. We will use AML Jupyter Widgets."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.train.estimator import Estimator\n\n# Define the location of the dataprep script and the location for the output bottleneck files\nscript_folder = 'script'\nscript_name = 'extract.py'\noutput_dir = 'bottleneck_features'\n\npip_packages = ['h5py', \n                'pillow', \n                'scipy',\n                'tensorflow-gpu']\n\nscript_params = {\n    '--data_folder': input_ds.as_download(),\n    '--output_data_dir': output_dir,\n    '--output_file_suffix': 'aerial_bottleneck_keras.h5'\n}\n\nest = Estimator(source_directory=script_folder,\n                script_params=script_params,\n                compute_target=compute_target,\n                entry_script=script_name,\n                node_count=1,\n                process_count_per_node=1,\n                use_gpu=True,\n                pip_packages=pip_packages,\n                inputs=[output_ds.path(output_dir).as_upload(path_on_compute=output_dir)])",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Submit the run and start RunDetails widget."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.widgets import RunDetails\n\ntags = {\"Compute target\": \"AML Compute GPU\", \"DNN\": \"Keras ResNet50\"}\nrun = exp.submit(config=est, tags=tags)\n\nRunDetails(run).show()",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "75b6472eb6a4458fb76528ffa4bcf84c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', '…"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Block to wait till the run finishes.\n\nThe call to start the run is asynchronous, it returns a **Starting** state as soon as the job is started.\n\nThe first run takes longer. The subsequent runs, as long as the script dependencies don't change, are much faster.\n\nHere is what's happening while you wait:\n\n- **Image creation**: A Docker image is created matching the Python environment specified by the estimator. The image is uploaded to the workspace. This stage happens once for each Python environment since the container is cached for subsequent runs.  During image creation, logs are streamed to the run history. You can monitor the image creation progress using these logs.\n\n- **Running**: In this stage, the training script is sent to the remote Azure ML Compute, then the data in the default datastore is copied to the local storage on the cluster node , then the script is run. While the job is running, stdout and the ./logs directory are streamed to the run history. You can monitor the run's progress using these logs. \n\n- **Post-Processing**:  The ./outputs directory on the cluster node  is copied over to the run history in the workspace.\n\nYou can check the progress of a running job in multiple ways. Below cells demonstrate how to use a Jupyter widget as well as a `wait_for_completion` method. \n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can cancel the run with."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "#run.cancel()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "run.wait_for_completion(show_output=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "After the run, AML copied the output bottleneck files to the default datastore. You can verify it using Azure Portal."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Training\nThe run has completed. The next step is to train a small Fully Connected Neural Network using engineered bottleneck features.\n\nWe will use AML feature called `Hyperdrive` to fine tune hyperparameters of the neural network. `Hyperdrive` will utilize Azure ML Compute GPU cluster to run and evaluate concurrent training jobs. After the model is fine tuned, the best version will be registered in AML Model Registry.\n\n### Create training script\n\nIn the training script, we use Tensorflow.Keras to define and train a simple fully connected neural network.\n\nThe network has one hidden layer. The input to the network is a vector of 2048 floating point numbers - the bottleneck features created in the previous step. The output layer consists of 6 units - representing six land type classes. To control overfitting the network uses a Dropout layer between the hidden layer and the output layer and L1 and L2 regularization in the output layer.\n\nThe number of units in the hidden layer, L1 and L2 values, and batch size are all tuneable hyperparameters. The Dropout ratio is fixed at 0.5.\n\nSince the bottleneck feature files are small (as compared to original image datasets) they can be loaded into memory all at once.\n\nThe trained model will be saved into the ./outputs folder. This is one of the special folders in AML. The other one is the ./logs folder. The content in these folders is automatically uploaded to the run history.\n\nThe script uses AML Run object to track two performane measures: training accuracy and validation accuracy. The metrics are captured at the end of each epoch.\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "script_name = 'train.py'",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%writefile $script_folder/train.py\n\nimport os\nimport tensorflow as tf\nfrom tensorflow.keras.applications import resnet50\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Flatten, Input\nfrom tensorflow.keras.regularizers import l1_l2\n\nfrom azureml.core import Run\n\nimport numpy as np\nimport random\nimport h5py\n\n\n# Create custom callback to track accuracy measures in AML Experiment\nclass RunCallback(tf.keras.callbacks.Callback):\n    def __init__(self, run):\n        self.run = run\n        \n    def on_epoch_end(self, batch, logs={}):\n        self.run.log(name=\"training_acc\", value=float(logs.get('acc')))\n        self.run.log(name=\"validation_acc\", value=float(logs.get('val_acc')))\n\n\n# Define network\ndef fcn_classifier(input_shape=(2048,), units=512, classes=6,  l1=0.01, l2=0.01):\n    features = Input(shape=input_shape)\n    x = Dense(units, activation='relu')(features)\n    x = Dropout(0.5)(x)\n    y = Dense(classes, activation='softmax', kernel_regularizer=l1_l2(l1=l1, l2=l2))(x)\n    model = Model(inputs=features, outputs=y)\n    model.compile(optimizer='adadelta', loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n# Training regime\ndef train_evaluate(run):\n   \n    print(\"Loading bottleneck features\")\n    train_file_name = os.path.join(FLAGS.data_folder, FLAGS.train_file_name)\n    valid_file_name = os.path.join(FLAGS.data_folder, FLAGS.valid_file_name)\n    \n    # Load bottleneck training features and labels\n    with h5py.File(train_file_name, \"r\") as hfile:\n        X_train = np.array(hfile.get('features'))\n        y_train = np.array(hfile.get('labels'))\n        \n     # Load bottleneck validation features and labels\n    with h5py.File(valid_file_name, \"r\") as hfile:\n        X_validation = np.array(hfile.get('features'))\n        y_validation = np.array(hfile.get('labels'))\n        \n             \n    print(y_train.shape)\n    print(y_validation.shape)\n    \n    # Create a network\n    model = fcn_classifier(input_shape=(2048,), units=FLAGS.units, l1=FLAGS.l1, l2=FLAGS.l2)\n    \n    # Create AML tracking callback\n    run_callback = RunCallback(run)\n    \n    # Start training\n    print(\"Starting training\")\n    model.fit(X_train, y_train,\n          batch_size=FLAGS.batch_size,\n          epochs=FLAGS.epochs,\n          shuffle=True,\n          validation_data=(X_validation, y_validation),\n          callbacks=[run_callback])\n          \n    # Save the trained model to outputs which is a standard folder expected by AML\n    print(\"Training completed.\")\n    os.makedirs('outputs', exist_ok=True)\n    model_file = os.path.join('outputs', 'aerial_fcnn_classifier.h5')\n    print(\"Saving model to: {0}\".format(model_file))\n    model.save(model_file)\n    \n\nFLAGS = tf.app.flags.FLAGS\n\n# Default global parameters\ntf.app.flags.DEFINE_integer('batch_size', 32, \"Number of images per batch\")\ntf.app.flags.DEFINE_integer('epochs', 10, \"Number of epochs to train\")\ntf.app.flags.DEFINE_integer('units', 512, \"Number of epochs to train\")\ntf.app.flags.DEFINE_float('l1', 0.01, \"l1 regularization\")\ntf.app.flags.DEFINE_float('l2', 0.01, \"l2 regularization\")\ntf.app.flags.DEFINE_string('data_folder', './bottleneck', \"Folder with bottleneck features and labels\")\ntf.app.flags.DEFINE_string('train_file_name', 'train_aerial_bottleneck_keras.h5', \"Training file name\")\ntf.app.flags.DEFINE_string('valid_file_name', 'valid_aerial_bottleneck_keras.h5', \"Validation file name\")\n\n\ndef main(argv=None):\n    \n    # get hold of the current run\n    run = Run.get_submitted_run()\n    train_evaluate(run)\n  \n\nif __name__ == '__main__':\n    tf.app.run()\n    ",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Writing script/train.py\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Configure datastore\n\nThe bottleneck files have been uploaded to the workspace's default datastore during the previous step. We will mount the store on the nodes of the cluster.\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core import Datastore\n\nds = ws.get_default_datastore()\nprint(\"Using the default datastore for training data: \")\nprint(ds.name, ds.datastore_type, ds.account_name, ds.container_name)\n",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Using the default datastore for training data: \nworkspaceblobstore AzureBlob jkamlwsstorageijhxtdoi azureml-blobstore-68c45b05-3479-49da-8b04-2517f76acaf6\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Create and experiment"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core import Experiment\nexperiment_name = 'aerial-hyperdrive'\nexp = Experiment(workspace=ws, name=experiment_name)",
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Run a test run on a single node of the cluster"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.train.estimator import Estimator\n\nscript_params = {\n    '--data_folder': ds.path('bottleneck_features').as_download(),\n    '--train_file_name': 'train_aerial_bottleneck_keras.h5',\n    '--valid_file_name': 'valid_aerial_bottleneck_keras.h5',\n    '--l1': 0.001,\n    '--l2': 0.001,\n    '--units': 512,\n    '--epochs': 50\n}\n\npip_packages = ['h5py', \n                'pillow', \n                'scipy',\n                'tensorflow-gpu']\n\nest = Estimator(source_directory=script_folder,\n                script_params=script_params,\n                compute_target=compute_target,\n                entry_script=script_name,\n                node_count=1,\n                process_count_per_node=1,\n                use_gpu=True,\n                pip_packages=pip_packages,\n                )\n",
      "execution_count": 14,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "tags = {\"Compute target\": \"AML Compute\", \"Run Type\": \"Test drive\"}\nrun = exp.submit(est, tags=tags)\nrun",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 15,
          "data": {
            "text/html": "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>aerial-hyperdrive</td><td>aerial-hyperdrive_1549300990429</td><td>azureml.scriptrun</td><td>Queued</td><td><a href=\"https://mlworkspace.azure.ai/portal/subscriptions/952a710c-8d9c-40c1-9fec-f752138cc0b3/resourceGroups/jkamlws/providers/Microsoft.MachineLearningServices/workspaces/jkamlws/experiments/aerial-hyperdrive/runs/aerial-hyperdrive_1549300990429\" target=\"_blank\" rel=\"noopener\">Link to Azure Portal</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.script_run.ScriptRun?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>",
            "text/plain": "Run(Experiment: aerial-hyperdrive,\nId: aerial-hyperdrive_1549300990429,\nType: azureml.scriptrun,\nStatus: Queued)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.widgets import RunDetails\nRunDetails(run).show()",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "29d6b94274164a24b8217af78017d8f7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', '…"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Learning curves indicate that the network overfits with the above hyperparameter settings. "
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "run.wait_for_completion(show_output=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Configure `Hyperdrive`\n\nAs noted before, our network has 5 hyperparameters:\n\n- Number of units in the hidden layer\n- L1 and L2 regularization\n- mini-batch size, and\n- dropout ratio\n\nAs we have limited time to complete the lab, we are going to limit a number of hyperparameter combinations to try. We will use a fixed batch-size and dropout ratio and focus on hidden layer units and L1 and L2 regularization.\n\n*Hyperdrive* supports many strategies for sampling the hyperparameter space. In this lab, we are going to use the simplest one - grid sampling."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.train.hyperdrive import *\n\nps = GridParameterSampling(\n    {\n        '--units': choice(256, 512),\n        '--l1': choice(0.001, 0.01, 0.05),\n        '--l2': choice(0.001, 0.01, 0.05)\n    }\n)",
      "execution_count": 17,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We will use **Estimator** object to configure the training job. Note how we pass the location of the bottleneck files to the estimator. The job will run on GPU VMs and as such we need to use the GPU version of Tensorflow."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.train.estimator import Estimator\n\nscript_params = {\n    '--data_folder': ds.path('bottleneck_features').as_download(),\n    '--train_file_name': 'train_aerial_bottleneck_keras.h5',\n    '--valid_file_name': 'valid_aerial_bottleneck_keras.h5',\n    '--epochs': 50\n}\n\n\npip_packages = ['h5py', \n                'pillow', \n                'scipy',\n                'tensorflow-gpu']\n\n\nest = Estimator(source_directory=script_folder,\n                script_params=script_params,\n                compute_target=compute_target,\n                entry_script=script_name,\n                node_count=1,\n                process_count_per_node=1,\n                use_gpu=True,\n                pip_packages=pip_packages,\n                )\n",
      "execution_count": 18,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "*Hyperdrive* supports early termination policies to limit exploration of hyperparameter combinations that don't show promise of helping reach the target metric. This feature is especially useful when traversing large hyperparameter spaces. Since we are going to run a small number of jobs we will not apply early termination."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "policy = NoTerminationPolicy()",
      "execution_count": 19,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now we are ready to configure a run configuration object, and specify the primary metric as *validation_acc* that's recorded in our training runs. If you go back to visit the training script, you will notice that this value is being logged after every mini-batch. We also want to tell the service that we are looking to maximize this value. We also set the number of total runs to 12, and maximal concurrent job to 4, which is the same as the number of nodes in our computer cluster. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "htc = HyperDriveRunConfig(estimator=est, \n                          hyperparameter_sampling=ps,\n                          policy=policy,\n                          primary_metric_name='validation_acc', \n                          primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \n                          max_total_runs=12,\n                          max_concurrent_runs=4)",
      "execution_count": 20,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Finally, let's launch the hyperparameter tuning job.\n\nThe first run takes longer as the system has to prepare and deploy a docker image with training job runtime dependencies. As long as the dependencies don't change the following runs will be much faster.\n\nHere is what's happening whie you wait.\n\n- **Image creation**: A Docker image is created matching the Python environment specified by the estimator. The image is uploaded to the workspace. This stage happens once for each Python environment since the container is cached for subsequent runs. During image creation, logs are streamed to the run history. You can monitor the image creation progress using these logs.\n\n- **Scaling**: If the remote cluster requires more nodes to execute the run than currently available, additional nodes are added automatically.\n\n- **Running**: In this stage, the necessary scripts and files are sent to the compute target, then data stores are mounted/copied, then the entry_script is run. While the job is running, stdout and the ./logs directory are streamed to the run history. You can monitor the run's progress using these logs.\n\n- **Post-Processing**: The ./outputs directory of the run is copied over to the run history in your workspace so you can access these results.\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "tags = {\"Training\": \"Hyperdrive\"}\n\nhdr = exp.submit(config=htc, tags=tags)\nhdr",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 21,
          "data": {
            "text/html": "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>aerial-hyperdrive</td><td>aerial-hyperdrive_1549301074816</td><td>hyperdrive</td><td>Running</td><td><a href=\"https://mlworkspace.azure.ai/portal/subscriptions/952a710c-8d9c-40c1-9fec-f752138cc0b3/resourceGroups/jkamlws/providers/Microsoft.MachineLearningServices/workspaces/jkamlws/experiments/aerial-hyperdrive/runs/aerial-hyperdrive_1549301074816\" target=\"_blank\" rel=\"noopener\">Link to Azure Portal</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/overview/azure/ml/intro?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>",
            "text/plain": "Run(Experiment: aerial-hyperdrive,\nId: aerial-hyperdrive_1549301074816,\nType: hyperdrive,\nStatus: Running)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.widgets import RunDetails\nRunDetails(hdr).show()",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5605d8179351471fb864a83627252af5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": "_HyperDriveWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO'…"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "hdr.wait_for_completion(show_output=False) # specify True for a verbose log",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Find and register best model\nWhen all jobs finish, we can find out the one that has the highest accuracy."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "best_run = hdr.get_best_run_by_primary_metric()",
      "execution_count": 23,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "best_run_metrics = best_run.get_metrics()\nparameter_values = best_run.get_details()['runDefinition']['Arguments']\n\nprint('Best Run Id: ', best_run.id)\nprint('\\n Validation Accuracy:', best_run_metrics['validation_acc'])\nprint('\\n Units:',parameter_values[7])\nprint('\\n L1:',parameter_values[9])\nprint('\\n L2:',parameter_values[11])",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Best Run Id:  aerial-hyperdrive_1549301074816_6\n\n Validation Accuracy: [0.9288025886109732, 0.9352750805203582, 0.9401294494523971, 0.9449838183844359, 0.9449838183844359, 0.9417475724297434, 0.933656957543012, 0.9352750805203582, 0.9433656954070897, 0.9449838183844359, 0.9417475724297434, 0.9498381873164747, 0.9466019413617822, 0.9368932034977046, 0.9417475724297434, 0.9417475724297434, 0.9433656954070897, 0.9449838183844359, 0.9385113264750509, 0.9352750805203582, 0.9401294494523971, 0.9368932034977046, 0.9304207115883194, 0.9368932034977046, 0.9433656954070897, 0.9352750805203582, 0.9417475724297434, 0.9449838183844359, 0.9482200643391285, 0.9401294494523971, 0.9401294494523971, 0.9385113264750509, 0.9433656954070897, 0.9385113264750509, 0.9368932034977046, 0.933656957543012, 0.9368932034977046, 0.9417475724297434, 0.9433656954070897, 0.9433656954070897, 0.9433656954070897, 0.9433656954070897, 0.9417475724297434, 0.9352750805203582, 0.9352750805203582, 0.9401294494523971, 0.9368932034977046, 0.9401294494523971, 0.9352750805203582, 0.9433656954070897]\n\n Units: 50\n\n L1: 256\n\n L2: 0.001\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Check the output of the best run."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(best_run.get_file_names())",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['azureml-logs/55_batchai_execution.txt', 'azureml-logs/60_control_log.txt', 'azureml-logs/80_driver_log.txt', 'azureml-logs/azureml.log', 'outputs/aerial_fcnn_classifier.h5']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Register model\nThe last step in the training script wrote the file `aerial_fcnn_classifier.hd5` in the `outputs` directory. As noted before, `outputs` is a special directory in that all content in this  directory is automatically uploaded to your workspace.  This content appears in the run record in the experiment under your workspace. \n\nYou can register the model so that it can be later queried, examined and deployed."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "model = best_run.register_model(model_name='aerial_classifier', \n                                model_path='outputs/aerial_fcnn_classifier.h5')\nprint(model.name, model.id, model.version, sep = '\\t')",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": "aerial_classifier\taerial_classifier:1\t1\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Clean up resources\nIf you are not going to walk through the other labs, delete the cluster."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# compute_target.delete()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}