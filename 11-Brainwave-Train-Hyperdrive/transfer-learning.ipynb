{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note\n",
    "\n",
    "*We have recenty experienced issues with `azureml.contrib.brainwave` library. Till the issues are addressed we are suspending development of FPGA labs. These are the labs with **1x** prefix*.\n",
    "\n",
    "\n",
    "\n",
    "# Developing Custom Image Classification Model\n",
    "\n",
    "In this lab, you will developed a custom image classification model to automatically classify the type of land shown in aerial images of 224-meter x 224-meter plots. Land use classification models can be used to track urbanization, deforestation, loss of wetlands, and other major environmental trends using periodically collected aerial imagery. The images used in this lab are based off of imagery from the U.S. National Land Cover Database. U.S. National Land Cover Database defines six primary classes of land use: *Developed*, *Barren*, *Forested*, *Grassland*, *Shrub*, *Cultivated*. Example images from each land use class are shown here:\n",
    "\n",
    "Developed | Cultivated | Barren\n",
    "--------- | ------ | ----------\n",
    "![Developed](https://github.com/jakazmie/images-for-hands-on-labs/raw/master/developed1.png) | ![Cultivated](https://github.com/jakazmie/images-for-hands-on-labs/raw/master/cultivated1.png) | ![Barren](https://github.com/jakazmie/images-for-hands-on-labs/raw/master/barren1.png)\n",
    "\n",
    "Forested | Grassland | Shrub\n",
    "---------| ----------| -----\n",
    "![Forested](https://github.com/jakazmie/images-for-hands-on-labs/raw/master/forest1.png) | ![Grassland](https://github.com/jakazmie/images-for-hands-on-labs/raw/master/grassland1.png) | ![Shrub](https://github.com/jakazmie/images-for-hands-on-labs/raw/master/shrub1.png)\n",
    "\n",
    "You shall employ a machine learning technique called transfer learning. Transfer learning is one of the fastest (code and run-time-wise) ways to start using deep learning. It allows for the reuse of knowledge gained while solving one problem to a different but related problem. For example, knowledge gained while learning to recognize landmarks and landscapes could apply when trying to recognize aerial land plots. Transfer Learning makes it feasible to train very effective ML models on relatively small training data sets.\n",
    "\n",
    "Although the primary goal of this lab is to understand how to use Azure ML to orchestrate deep learning workflows rather then to dive into Deep Learning techniques, ask the instructor if you want to better understand the approach utilized in the lab.\n",
    "\n",
    "You will start by pre-processing training images into a set of powerful features - sometimes referred to as bottleneck features.\n",
    "\n",
    "To create bottleneck features you will utilize a pre-trained Deep Learning network that was trained on a general computer vision domain. \n",
    "\n",
    "Although, the pre-trained network does not know how to classify aerial land plot images, it knows enough about representing image concepts that if we use it to pre-process aerial images, the extracted image features can be used to effectively train a relatively simple classifier on a **limited number** of samples.\n",
    "\n",
    "The below diagram represents the architecture of our solution.\n",
    "\n",
    "![Transfer Learning](https://github.com/jakazmie/images-for-hands-on-labs/raw/master/TLArch.png)\n",
    "\n",
    "We will use **ResNet50** trained on **imagenet** dataset to extract features. We will occasionally refer to this component of the solution as a featurizer. The output of the featurizer is a vector of 2048 floating point numbers, each representing a feature extracted from an image. \n",
    "\n",
    "We will then use extracted features to train a simple fully connected neural network (the top) that will peform final image classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to AML Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Azure ML Managed Compute\n",
    "\n",
    "To run the lab's scripts we will utilize Azure ML managed compute resources. Specifically, an autoscale cluster of *Standard_NC6* VMs (equipped with Tesla K80 GPU). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "import os\n",
    "\n",
    "\n",
    "# choose a name for your cluster\n",
    "compute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"gpu-cluster\")\n",
    "compute_min_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MIN_NODES\", 1)\n",
    "compute_max_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MAX_NODES\", 4)\n",
    "\n",
    "vm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_NC6\")\n",
    "#vm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_ND6s\")\n",
    "\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print('found compute target. just use it. ' + compute_name)\n",
    "else:\n",
    "    print('creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size,\n",
    "                                                                min_nodes = compute_min_nodes, \n",
    "                                                                max_nodes = compute_max_nodes)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
    "\n",
    "    # can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # if no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    "     # For a more detailed view of current AmlCompute status, use the 'status' property    \n",
    "    print(compute_target.status.serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "As noted in the introduction, we will utilize a pretrained **ResNet50** convolutional neural net as a featurizer. This CNN was trained on *Imagenet* dataset. We will use the version of **ResNet50** from `azureml.contrib.brainwave.models` library. The models in this library are optimized for inference on **FPGA** devices (Project Brainwave). In the following labs you deploy this model into an FGPA backed cloud service.\n",
    "\n",
    "The feature extraction code is encapsulated in a Python script that will be executed on a remote Azure ML Compute.\n",
    "\n",
    "The script processes an input image dataset into an output bottleneck feature set. The script expects the images to be organized in the below folder structure:\n",
    "```\n",
    "Barren/\n",
    "Cultivated/\n",
    "Developed/\n",
    "Forest/\n",
    "Herbaceous/\n",
    "Shrub/\n",
    "```\n",
    "\n",
    "The location of the input dataset and the location where to save the output dataset are passed to the script as command line parameters. The output dataset will be stored in a binary HDF5 data format used commonly in Machine Learning and High Performance Computing solutions.\n",
    "\n",
    "The script is designed to work with a large number of images. As such it does not load all input images to memory at once. Instead it utilizes a utility function `load_images` to feed the featurizer. The function yields batches of images - as Numpy arrays - preprocessed to the format required by **ResNet50**. \n",
    "\n",
    "We will not attempt to run the script on a full dataset in a local environment. It is very computationally intensive and unless you run it in an evironment equipped with a powerful GPU it would be very slow. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a feature extraction script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "script_folder = './script'\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $script_folder/extract.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "from random import shuffle\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import azureml.contrib.brainwave.models.utils as utils\n",
    "from azureml.contrib.brainwave.models import QuantizedResnet50\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_batch(pathnames, batchsize=64):\n",
    "    \"\"\"Yield succesive batches of images\"\"\"\n",
    "    for i in range(0, len(pathnames), batchsize):\n",
    "        yield pathnames[i:i+batchsize]\n",
    "        \n",
    "def load_images(batch):\n",
    "    \"\"\"Return a batch of images as a list of bytes sequences\"\"\"\n",
    "    images = []\n",
    "    for path in batch:\n",
    "        with open(path, 'rb') as f:\n",
    "            images.append(f.read())\n",
    "    return images\n",
    "\n",
    "def create_bottleneck_features():\n",
    "    \"\"\"Createl bottleneck features and save them to H5 formatted file\"\"\"\n",
    "    img_dir = FLAGS.input_data_dir\n",
    "    \n",
    "    # Label images \n",
    "    \n",
    "    # Create the dictionary that maps class names into numeric labels   \n",
    "    label_map = {\n",
    "        \"Barren\": 0,\n",
    "        \"Cultivated\": 1,\n",
    "        \"Developed\": 2,\n",
    "        \"Forest\": 3,\n",
    "        \"Herbaceous\": 4,\n",
    "        \"Shrub\": 5}    \n",
    "\n",
    "    # Create a list of all images in a root folder with associated numeric labels\n",
    "    folders = list(label_map.keys())\n",
    "    labeled_image_list = [(os.path.join(img_dir, folder, image), label_map[folder]) \n",
    "                          for folder in folders \n",
    "                          for image in os.listdir(os.path.join(img_dir, folder))\n",
    "                              ]\n",
    "    # Shuffle the list\n",
    "    random.shuffle(labeled_image_list)\n",
    "    image_paths, labels = zip(*labeled_image_list)\n",
    "    \n",
    "    # Build featurizer graph\n",
    "    \n",
    "    # Convert input images (loaded as bytes sequences) into (224, 224, 3) tensors\n",
    "    # with pixel values in Caffe encoding\n",
    "    in_images = tf.placeholder(tf.string)\n",
    "    image_tensors = utils.preprocess_array(in_images)\n",
    "\n",
    "    # Create ResNet50 \n",
    "    model_path = os.path.expanduser('~/models')\n",
    "    resnet = QuantizedResnet50(model_path, is_frozen=True)\n",
    "\n",
    "    # Import ResNet50 graph\n",
    "    features = resnet.import_graph_def(input_tensor=image_tensors)\n",
    "    \n",
    "    # Generate bottleneck features\n",
    "    print(\"Generating bottleneck features\")\n",
    "    bottleneck_features = []\n",
    "    with tf.Session() as sess:\n",
    "        for paths in tqdm(get_batch(image_paths)):\n",
    "            image_batch = load_images(paths)\n",
    "            result = sess.run([features], feed_dict={in_images: image_batch})\n",
    "            result = np.reshape(result[0], (len(result[0]), 2048))\n",
    "            bottleneck_features.extend(result)\n",
    "        \n",
    "        \n",
    "    bottleneck_features = np.asarray(bottleneck_features)\n",
    "    labels = np.asarray(labels)\n",
    "    print(\"Features shape: \",bottleneck_features.shape)\n",
    "    print(\"Labels shape: \", labels.shape)\n",
    "    \n",
    "    # Split the data into training and validation partitions   \n",
    "    bottleneck_train, bottleneck_valid, labels_train, labels_valid = \\\n",
    "        train_test_split(bottleneck_features, labels,\n",
    "        test_size=0.1,\n",
    "        shuffle=True,\n",
    "        stratify=labels)\n",
    "\n",
    "  \n",
    "    # Save the bottleneck features to HDF5 files\n",
    "\n",
    "    for prefix, features, labels in [('train', bottleneck_train, labels_train), ('valid', bottleneck_valid, labels_valid)]:\n",
    "        filename = os.path.join(FLAGS.output_data_dir, prefix + '_' + FLAGS.file_name)\n",
    "        print(\"Saving bottleneck features to {}\".format(filename))\n",
    "        print(\"   Features: \", features.shape)\n",
    "        print(\"   Labels: \", labels.shape)\n",
    "        with h5py.File(filename, \"w\") as hfile:\n",
    "            features_dset = hfile.create_dataset('features', data=features)\n",
    "            labels_dset = hfile.create_dataset('labels', data=labels)\n",
    "    \n",
    "    print(\"Done\")\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "# Default global parameters\n",
    "tf.app.flags.DEFINE_integer('batch_size', 64, \"Number of images per batch\")\n",
    "tf.app.flags.DEFINE_string('input_data_dir', 'aerialtiny', \"Folder with training and validation images\")\n",
    "tf.app.flags.DEFINE_string('output_data_dir', 'bottleneck_features', \"A folder for saving bottleneck features\")\n",
    "tf.app.flags.DEFINE_string('file_name', 'aerial_bottleneck_resnet50.h5', \"Name of output training file\")\n",
    "\n",
    "\n",
    "def main(argv=None):\n",
    "    print(\"Starting\")\n",
    "    print(\"Reading images from:\", FLAGS.input_data_dir)\n",
    "    print(\"The output bottleneck file will be saved to:\", FLAGS.output_data_dir)\n",
    "\n",
    "    os.makedirs(FLAGS.output_data_dir, exist_ok=True)\n",
    "\n",
    "    create_bottleneck_features()\n",
    "  \n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Datastores \n",
    "The training images have been uploaded to a public Azure blob storage container. We will register this container as an AML Datastore within our workspace. Before the data prep script runs, the datastore's content - training images - will be copied to the local storage on compute nodes.\n",
    "\n",
    "After the script completes, its output - the bottleneck features file - will be uploaded by AML to the workspace's default datastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Datastore\n",
    "\n",
    "images_account = 'azureailabs'\n",
    "images_container = 'aerial-small'\n",
    "datastore_name = 'input_images'\n",
    "\n",
    "# Check if the datastore exists. If not create a new one\n",
    "try:\n",
    "    input_ds = Datastore.get(ws, datastore_name)\n",
    "    print('Found existing datastore for input images:', input_ds.name)\n",
    "except:\n",
    "    input_ds = Datastore.register_azure_blob_container(workspace=ws, datastore_name=datastore_name,\n",
    "                                            container_name=images_container,\n",
    "                                            account_name=images_account)\n",
    "    print('Creating new datastore for input images')\n",
    "\n",
    " \n",
    "   \n",
    "print(input_ds.name, input_ds.datastore_type, input_ds.account_name, input_ds.container_name)\n",
    "\n",
    "output_ds = ws.get_default_datastore()\n",
    "print(\"Using the default datastore for output: \")\n",
    "print(output_ds.name, output_ds.datastore_type, output_ds.account_name, output_ds.container_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create AML Experiment\n",
    "We will track runs of the feature extraction script in a dedicated Experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "experiment_name = 'aerial-feature-extraction'\n",
    "exp = Experiment(workspace=ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start and monitor a remote run\n",
    "\n",
    "We will run a script on a single node in a docker container. The docker image will be configured and created using AML APIs.\n",
    "\n",
    "The first run takes longer. The subsequent runs, as long as the script dependencies don't change, are much faster.\n",
    "\n",
    "You can check the progress of a running job in multiple ways: Azure Portal, AML Jupyter Widgets, log files streaming. We will use AML Jupyter Widgets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.estimator import Estimator\n",
    "\n",
    "# Define the location of the dataprep script and the location for the output bottleneck files\n",
    "script_folder = 'script'\n",
    "script_name = 'extract.py'\n",
    "output_dir = 'bottleneck_features'\n",
    "\n",
    "pip_packages = ['h5py', \n",
    "                'pillow', \n",
    "                'tqdm', \n",
    "                'azureml-sdk[contrib]', \n",
    "                'scikit-learn', \n",
    "                'tensorflow-gpu==1.10']\n",
    "\n",
    "script_params = {\n",
    "    '--input_data_dir': input_ds.as_download(),\n",
    "    '--output_data_dir': output_dir,\n",
    "    '--file_name': 'aerial_bottleneck_resnet50_brainwave.h5'\n",
    "}\n",
    "\n",
    "est = Estimator(source_directory=script_folder,\n",
    "                script_params=script_params,\n",
    "                compute_target=compute_target,\n",
    "                entry_script=script_name,\n",
    "                node_count=1,\n",
    "                process_count_per_node=1,\n",
    "                use_gpu=True,\n",
    "                pip_packages=pip_packages,\n",
    "                inputs=[output_ds.path(output_dir).as_upload(path_on_compute=output_dir)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit the run and start RunDetails widget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "\n",
    "tags = {\"Compute target\": \"AML Compute GPU\", \"DNN\": \"Brainwave ResNet50\"}\n",
    "run = exp.submit(config=est, tags=tags)\n",
    "\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Block to wait till the run finishes.\n",
    "\n",
    "The call to start the run is asynchronous, it returns a **Starting** state as soon as the job is started.\n",
    "\n",
    "The first run takes longer. The subsequent runs, as long as the script dependencies don't change, are much faster.\n",
    "\n",
    "Here is what's happening while you wait:\n",
    "\n",
    "- **Image creation**: A Docker image is created matching the Python environment specified by the estimator. The image is uploaded to the workspace. This stage happens once for each Python environment since the container is cached for subsequent runs.  During image creation, logs are streamed to the run history. You can monitor the image creation progress using these logs.\n",
    "\n",
    "- **Running**: In this stage, the training script is sent to the remote Azure ML Compute, then the data in the default datastore is copied to the local storage on the cluster node , then the script is run. While the job is running, stdout and the ./logs directory are streamed to the run history. You can monitor the run's progress using these logs. \n",
    "\n",
    "- **Post-Processing**:  The ./outputs directory on the cluster node  is copied over to the run history in the workspace.\n",
    "\n",
    "You can check the progress of a running job in multiple ways. Below cells demonstrate how to use a Jupyter widget as well as a `wait_for_completion` method. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the run, AML copied the output bottleneck files to the default datastore. You can verify it using Azure Portal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "The run has completed. The next step is to train a small Fully Connected Neural Network using engineered bottleneck features.\n",
    "\n",
    "After the model is trained, it will be registered in AML Model Registry.\n",
    "\n",
    "### Create training script\n",
    "\n",
    "In the training script, we use Tensorflow.Keras to define and train a simple fully connected neural network.\n",
    "\n",
    "The network has one hidden layer. The input to the network is a vector of 2048 floating point numbers - the bottleneck features created in the previous step. The output layer consists of 6 units - representing six land type classes. To control overfitting the network uses a Dropout layer between the hidden layer and the output layer and L1 and L2 regularization in the output layer.\n",
    "\n",
    "The number of units in the hidden layer, L1 and L2 values, and batch size are all tuneable hyperparameters. The Dropout ratio is fixed at 0.5.\n",
    "\n",
    "Since the bottleneck feature files are small (as compared to original image datasets) they can be loaded into memory all at once.\n",
    "\n",
    "The trained model will be saved into the ./outputs folder. This is one of the special folders in AML. The other one is the ./logs folder. The content in these folders is automatically uploaded to the run history.\n",
    "\n",
    "The script uses AML Run object to track two performane measures: training accuracy and validation accuracy. The metrics are captured at the end of each epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_name = 'train.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $script_folder/train.py\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import resnet50\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Flatten, Input\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from azureml.core import Run\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import h5py\n",
    "\n",
    "\n",
    "# Create custom callback to track accuracy measures in AML Experiment\n",
    "class RunCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, run):\n",
    "        self.run = run\n",
    "        \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.run.log(name=\"training_acc\", value=float(logs.get('acc')))\n",
    "        self.run.log(name=\"validation_acc\", value=float(logs.get('val_acc')))\n",
    "\n",
    "\n",
    "# Define network\n",
    "def fcn_classifier(input_shape=(2048,), units=512, classes=6,  l1=0.01, l2=0.01):\n",
    "    features = Input(shape=input_shape)\n",
    "    x = Dense(units, activation='relu')(features)\n",
    "    x = Dropout(0.5)(x)\n",
    "    y = Dense(classes, activation='softmax', kernel_regularizer=l1_l2(l1=l1, l2=l2))(x)\n",
    "    model = Model(inputs=features, outputs=y)\n",
    "    model.compile(optimizer='adadelta', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Training regime\n",
    "def train_evaluate(run):\n",
    "   \n",
    "    print(\"Loading bottleneck features\")\n",
    "    train_file_name = os.path.join(FLAGS.data_folder, FLAGS.train_file_name)\n",
    "    valid_file_name = os.path.join(FLAGS.data_folder, FLAGS.valid_file_name)\n",
    "    \n",
    "    # Load bottleneck training features and labels\n",
    "    with h5py.File(train_file_name, \"r\") as hfile:\n",
    "        X_train = np.array(hfile.get('features'))\n",
    "        y_train = np.array(hfile.get('labels'))\n",
    "        \n",
    "     # Load bottleneck validation features and labels\n",
    "    with h5py.File(valid_file_name, \"r\") as hfile:\n",
    "        X_validation = np.array(hfile.get('features'))\n",
    "        y_validation = np.array(hfile.get('labels'))\n",
    "                \n",
    "    print(y_train.shape)\n",
    "    print(y_validation.shape)\n",
    "    # Convert labels into one-hot encoded format\n",
    "    y_train = to_categorical(y_train, num_classes=6)\n",
    "    y_validation = to_categorical(y_validation, num_classes=6)\n",
    "    \n",
    "    # Create a network\n",
    "    model = fcn_classifier(input_shape=(2048,), units=FLAGS.units, l1=FLAGS.l1, l2=FLAGS.l2)\n",
    "    \n",
    "    # Create AML tracking callback\n",
    "    run_callback = RunCallback(run)\n",
    "    \n",
    "    # Start training\n",
    "    print(\"Starting training\")\n",
    "    model.fit(X_train, y_train,\n",
    "          batch_size=FLAGS.batch_size,\n",
    "          epochs=FLAGS.epochs,\n",
    "          shuffle=True,\n",
    "          validation_data=(X_validation, y_validation),\n",
    "          callbacks=[run_callback])\n",
    "          \n",
    "    # Save the trained model to outputs which is a standard folder expected by AML\n",
    "    print(\"Training completed.\")\n",
    "    os.makedirs('outputs', exist_ok=True)\n",
    "    model_file = os.path.join('outputs', 'aerial_fcnn_classifier_brainwave.h5')\n",
    "    print(\"Saving model to: {0}\".format(model_file))\n",
    "    model.save(model_file)\n",
    "    \n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "# Default global parameters\n",
    "tf.app.flags.DEFINE_integer('batch_size', 32, \"Number of images per batch\")\n",
    "tf.app.flags.DEFINE_integer('epochs', 10, \"Number of epochs to train\")\n",
    "tf.app.flags.DEFINE_integer('units', 512, \"Number of epochs to train\")\n",
    "tf.app.flags.DEFINE_float('l1', 0.01, \"l1 regularization\")\n",
    "tf.app.flags.DEFINE_float('l2', 0.01, \"l2 regularization\")\n",
    "tf.app.flags.DEFINE_string('data_folder', './bottleneck', \"Folder with bottleneck features and labels\")\n",
    "tf.app.flags.DEFINE_string('train_file_name', 'train_aerial_bottleneck_resnet50_brainwave.h5', \"Training file name\")\n",
    "tf.app.flags.DEFINE_string('valid_file_name', 'valid_aerial_bottleneck_resnet50_brainwave.h5', \"Validation file name\")\n",
    "\n",
    "\n",
    "def main(argv=None):\n",
    "    \n",
    "    # get hold of the current run\n",
    "    run = Run.get_submitted_run()\n",
    "    train_evaluate(run)\n",
    "  \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure datastore\n",
    "\n",
    "The bottleneck files have been uploaded to the workspace's default datastore during the previous step. We will mount the store on the nodes of the cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Datastore\n",
    "\n",
    "ds = ws.get_default_datastore()\n",
    "print(\"Using the default datastore for training data: \")\n",
    "print(ds.name, ds.datastore_type, ds.account_name, ds.container_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "experiment_name = 'aerial-brainwave-train'\n",
    "exp = Experiment(workspace=ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a test run on a single node of the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.estimator import Estimator\n",
    "\n",
    "script_params = {\n",
    "    '--data_folder': ds.path('bottleneck_features').as_download(),\n",
    "    '--train_file_name': 'train_aerial_bottleneck_resnet50_brainwave.h5',\n",
    "    '--valid_file_name': 'valid_aerial_bottleneck_resnet50_brainwave.h5',\n",
    "    '--l1': 0.001,\n",
    "    '--l2': 0.001,\n",
    "    '--units': 512,\n",
    "    '--epochs': 50\n",
    "}\n",
    "\n",
    "\n",
    "pip_packages = ['h5py', \n",
    "                'pillow', \n",
    "                'tqdm', \n",
    "                'azureml-sdk[contrib]', \n",
    "                'scikit-learn', \n",
    "                'tensorflow-gpu==1.10']\n",
    "\n",
    "est = Estimator(source_directory=script_folder,\n",
    "                script_params=script_params,\n",
    "                compute_target=compute_target,\n",
    "                entry_script=script_name,\n",
    "                node_count=1,\n",
    "                process_count_per_node=1,\n",
    "                use_gpu=True,\n",
    "                pip_packages=pip_packages,\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = {\"Compute target\": \"AML Compute\", \"Run Type\": \"Test drive\"}\n",
    "run = exp.submit(est, tags=tags)\n",
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning curves indicate that the network overfits with the above hyperparameter settings. \n",
    "\n",
    "To optimize the model you should fine tune hyperparameters as demonstrated in Lab 01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the output of the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(run.get_file_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register model\n",
    "The last step in the training script wrote the file `aerial_fcnn_classifier.hd5` in the `outputs` directory. As noted before, `outputs` is a special directory in that all content in this  directory is automatically uploaded to your workspace.  This content appears in the run record in the experiment under your workspace. \n",
    "\n",
    "You can register the model so that it can be later queried, examined and deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = run.register_model(model_name='aerial_classifier_brainwave', \n",
    "                                model_path='outputs/aerial_fcnn_classifier_brainwave.h5')\n",
    "print(model.name, model.id, model.version, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up resources\n",
    "Before you move to the next step, delete the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_target.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
